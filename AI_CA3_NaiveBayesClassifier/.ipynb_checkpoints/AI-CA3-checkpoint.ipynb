{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from math import log\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer \n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Computer Assignment 3\n",
    "### ARASH HATEFI / Student No: 810098016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim of Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we apply the Naïve Bayes classifier for categorizing three types of emails based on their contents. Each email would be classified as related to one of the \"Travel\", \"Business\", or \"Style & Beauty\" categories. In the beginning, we try to separate emails from the first two mentioned categories using Naïve Bayes, and finally, we will apply the method for classifying the three categories.\n",
    "\n",
    "The methodology and codes are all explained throughout this report and the effect of several related parameters and techniques are discussed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Access to CA's Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#Q1\">Question 1: Stemming vs. Lemmatization</a><br />\n",
    "<a href=\"#Q2\">Question 2: Using TF-IDF with the Naïve Bayes classifier</a><br />\n",
    "<a href=\"#Q3\">Question 3: The reason why precision is not enough for evaluating a classifier</a><br />\n",
    "<a href=\"#Q4\">Question 4: The impact of a single word which appeared only once in one of the training classes on</a><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Naïve Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naïve Bayes classifiers** are simple probabilistic classifiers based on **applying Bayes' theorem with strong independence assumptions** between the features. They are among the simplest Bayesian network models.\n",
    "\n",
    "\n",
    "Given a problem instance to be classified, represented by a vector $ {x} =(x_{1},\\ldots ,x_{n})$ representing some n features (independent variables), Naive Bayes assigns to this instance probabilities \n",
    "\n",
    "$$P ( C_k ∣ x_1,\\ldots, x_n )$$\n",
    "    \n",
    "for each of K possible outcomes or classes $C_k$. The instance is then classified as $C_k$, while $k$ is the class index which maximizes the above conditional probability.\n",
    "\n",
    "$$Class = \\underset{C_k}{\\operatorname{argmax}}P( C_k ∣ x_1,\\ldots, x_n )\\qquad(1)$$\n",
    "\n",
    "For computing the above expressions, the classifier will apply the Bayes' theorem to each of the probabilities.\n",
    "\n",
    "Bayes' theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event and is stated as follow:\n",
    "\n",
    "$$P(C_k∣x) = \\frac {P(C_k)}{P(x)}P(x ∣ C_k)$$\n",
    "\n",
    "Using the Bayes' rule, the conditional probabilities can be extended:\n",
    "\n",
    "$P( C_k ∣ x 1,\\ldots, x_n ) = [\\frac{ P(C_K)}{P(x_1,\\ldots, x_n )}]\\ P( C_k, x_1,\\ldots, x_n )= [\\frac{ P(C_K)}{P(x_1,\\ldots, x_n )}]\\ P(x_1,\\ldots, x_n , C_k) $\n",
    "\n",
    "$\\qquad= [\\frac{ P(C_K)}{P(x_1,\\ldots, x_n )}]\\ P(x_1 | x_2,\\ldots, x_n , C_k)\\ P(x_2,\\ldots, x_n , C_k)$\n",
    "\n",
    "$\\qquad=[\\frac{ P(C_K)}{P(x_1,\\ldots, x_n )}]\\ P(x_1 | x_2,\\ldots, x_n , C_k)\\ P(x_2 | x_3,\\ldots, x_n , C_k)\\ P(x_3,\\ldots, x_n , C_k)$\n",
    "\n",
    "$\\qquad=\\ldots=[\\frac{ P(C_K)}{P(x_1,\\ldots, x_n )}]\\ P(x_1 | x_2,\\ldots, x_n , C_k)\\ P(x_2 | x_3,\\ldots, x_n , C_k)\\ldots P(x_n | C_k)\\qquad(2)$\n",
    "\n",
    "\n",
    "\n",
    "Now the \"naive\" conditional independence assumptions come into play: assume that all features in $x$ are mutually independent, conditional on the category $C_k$. Under this assumption,\n",
    "\n",
    "$$ P ( x i ∣ x_{i + 1} , … , x_n , C_k ) = P ( x_i ∣ C_k ),\\qquad i={1,2,\\ldots,n}\\qquad(3)$$\n",
    "\n",
    "Thus, equation (2) can be expressed as\n",
    "\n",
    "$$P( C_k ∣ x 1,\\ldots, x_n ) = \\frac{ P(C_K)}{P(x_1,\\ldots, x_n )} \\prod_{i=0}^{n}{P( x_i ∣ C_k )}\\qquad(4)$$\n",
    "\n",
    "Using equation (4), we can rewrite equation (1) as follow:\n",
    "\n",
    "$$Class = \\underset{C_k}{\\operatorname{argmax}}\\frac{ P(C_K)}{P(x_1,\\ldots, x_n )} \\prod_{i=0}^{n}{P( x_i ∣ C_k )}$$\n",
    "\n",
    "\n",
    "The term $P(x_1,\\ldots, x_n )$ is appeadred for all values of $k$ in the above expression and so it does not affect finding the class of the instance. By removing it, we get:\n",
    "\n",
    "$$Class = \\underset{C_k}{\\operatorname{argmax}}P(C_K) \\prod_{i=0}^{n}{P( x_i ∣ C_k )}\\qquad(5)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words model is a very common feature extraction procedure for sentences and documents. In this approach, we look at the histogram of the words within the text, i.e. considering each word count as a feature. It is called a \"bag\" of words as any information about the order or structure of words in the document is discarded and the model is only concerned with whether known words occur in the document, not where in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of about 25500 emails and their relative information including \"**authors**\", \"**date**\", \"**headline**\", \"**link**\", and \"**short description**\". Approximately 23000 of the emails are labeled as one of \"**Travel**\", \"**Business**\", or \"**Style and Beauty**\" categories while others are unlabeled. The labeled and unlabeled emails are stored in two different CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELED_DATA_PATH = \"./data.csv\"\n",
    "UNLABELED_DATA_PATH = \"./test.csv\"\n",
    "\n",
    "raw_labeled_data = pd.read_csv(LABELED_DATA_PATH)\n",
    "raw_unlabeled_data = pd.read_csv(UNLABELED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 10 rows of the raw labeled data are as follow:\n",
    "\n",
    "raw_labeled_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the first 10 rows of the raw unlabeled data are as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Kate Middleton Has Not One But Two Style Wins ...</td>\n",
       "      <td>Jamie Feldman</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/kate-midd...</td>\n",
       "      <td>Now, we're not exactly saying Kate is cutting ...</td>\n",
       "      <td>2014-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Instagram Local Lens Series Features Insider's...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/instagram...</td>\n",
       "      <td>Instagram's Local Lens series is the perfect w...</td>\n",
       "      <td>2013-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Where To Go This Thanksgiving</td>\n",
       "      <td>Fodor's, ContributorFodors.com</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/where-to-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Retailers Hiring The Most Employees For The Ho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/retailers...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-10-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How To Get Flat Iron Waves In Under 2 Minutes ...</td>\n",
       "      <td>Simone Kitchens</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/flat-iron...</td>\n",
       "      <td>Check out the video on how to get our favorite...</td>\n",
       "      <td>2012-04-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           headline  \\\n",
       "0      0  Kate Middleton Has Not One But Two Style Wins ...   \n",
       "1      1  Instagram Local Lens Series Features Insider's...   \n",
       "2      2                      Where To Go This Thanksgiving   \n",
       "3      3  Retailers Hiring The Most Employees For The Ho...   \n",
       "4      4  How To Get Flat Iron Waves In Under 2 Minutes ...   \n",
       "\n",
       "                          authors  \\\n",
       "0                   Jamie Feldman   \n",
       "1                             NaN   \n",
       "2  Fodor's, ContributorFodors.com   \n",
       "3                             NaN   \n",
       "4                 Simone Kitchens   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/kate-midd...   \n",
       "1  https://www.huffingtonpost.com/entry/instagram...   \n",
       "2  https://www.huffingtonpost.com/entry/where-to-...   \n",
       "3  https://www.huffingtonpost.com/entry/retailers...   \n",
       "4  https://www.huffingtonpost.com/entry/flat-iron...   \n",
       "\n",
       "                                   short_description        date  \n",
       "0  Now, we're not exactly saying Kate is cutting ...  2014-04-10  \n",
       "1  Instagram's Local Lens series is the perfect w...  2013-12-30  \n",
       "2                                                NaN  2014-09-22  \n",
       "3                                                NaN  2014-10-05  \n",
       "4  Check out the video on how to get our favorite...  2012-04-25  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_unlabeled_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Removing extra Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was mentioned in section 2, the classification process is based on the emails' descriptions. Therefore, all the other irrelevant information like authors, date, link, etc. will be removed from the dataset. In the labeled dataset, the only remaining information about each email is its short description and category while in the unlabeled dataset, all extra information except the indices and the short descriptions are removed. \n",
    "\n",
    "Also, there are some defects in some of the dataset instances. To avoid facing problems, we remove all instances with defects from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = raw_labeled_data[['category', 'short_description']].dropna()\n",
    "unlabeled_data = raw_unlabeled_data[['index', 'short_description']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Processing the Short Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was discussed in section 3, the bag of words model deals with individual words in a text. Therefore, in the beginning, each email's description is split into its words. \n",
    "\n",
    "Some sources of irrelevancy exist in descriptions and for increasing the performance of the algorithm, it is important to remove or at least reduce these sources. In the two next sections, we will discuss different sources of irrelevancy and ways for removing them from the descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Irrelevancies in  Emails' Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important fact to deal with is the irrelevancies in descriptions. These irrelevancies my harder the task of classification for the algorithm as well as increasing the computational expenses. Three of the probable forms of irrelevancies are as follow:\n",
    "\n",
    "1. **Upper Case and Lower Case characters**: Despite their same meaning, words with mixed uppercase and lowercase letters vary form each other in shape and Naïve Bayes makes difference between them. So it is expected that changing all the letters in descriptions to their lowercase form raises the efficiency of the classifier.\n",
    "\n",
    "\n",
    "2. **Stop Words**: Stop words are a set of commonly used words in any language. For example, in English, \"the\", \"is\", and \"and\" would easily qualify as stop words. In language text processing tasks, these frequent words would be removed as they usually appear in all kinds of texts regardless of their category and carry no useful information for classifying texts.\n",
    "\n",
    "\n",
    "3. **Inflections**: For grammatical reasons, descriptions use different inflections of words. Despite their different appearance, inflections of a root word convey similar meanings. In the email classification problem, the classifier acts based on the similarity of words between a given instance and each class. Consequently, by reducing each word to its root form, the variety of words will decrease and the classifier is expected to perform better.\n",
    "\n",
    "We will discuss the effect of removing each of the mentioned probable irrelevancies on the final performance of the algorithm.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Ways of Removing Irrelevancies from Emails' Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, some methods are introduced for removing the discussed sources of irrelevancy in the previous section.\n",
    "\n",
    "Dealing with diversity in the shape of words caused by lowercase and uppercase letters is rather easy. We change all the letters in the descriptions to their lowercase form. Also, for finding the stop words in the descriptions, we need a set of English stopwords that can be provided form the Python **Natural Language Tool Kit (NLTK) library**. The stop words can easily be found and removed from the descriptions.\n",
    "\n",
    "\n",
    "In the case of the inflections, there are two main methods for removing words inflections:\n",
    "\n",
    "1. **Stemming**: Stemming is the process of reducing inflection in words to their root form such as mapping a group of words to the same stem even if the stem is not a valid word in the language.\n",
    "\n",
    "\n",
    "2. **Lemmatization**: Lemmatization is the process of replacing words with their lemma (the base or dictionary form of a word) depending on their meaning.\n",
    "\n",
    "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "we will use both of the mentioned methods for removing the infection words and measure the effect of both on the final results of the classifier. \n",
    "\n",
    "Both of the mentioned methods for dealing with infections are provided in the NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Removing Irrelevancies form Emails' Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function bellow gets a list of the descriptions and returns the processed texts. Each processing step is specified by comment and each processing stage can be removed by commenting its corresponding line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_descriptions(description_list):\n",
    "    \n",
    "    description_list = list(description_list)\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    ps = PorterStemmer()\n",
    "    ls = LancasterStemmer()\n",
    "    english_stopwords = stopwords.words(\"english\")\n",
    "    \n",
    "    # Converting each description to a list of its words\n",
    "    description_list = [tokenizer.tokenize(description) for description in description_list]\n",
    "    \n",
    "    # Removing the stop words for the lists\n",
    "    #description_list = [[word for word in description if not word in english_stopwords] for description in description_list]\n",
    "    \n",
    "    # Removing inflections using the stemming method\n",
    "    description_list = [[ps.stem(word) for word in description] for description in description_list]\n",
    "    \n",
    "    # Removing inflections using the Lemmatization method\n",
    "    description_list = [[ls.stem(word) for word in description] for description in description_list]\n",
    "    \n",
    "    # Converting all the characters to their lowercase form\n",
    "    description_list = [[word.lower() for word in description] for description in description_list]\n",
    "    \n",
    "    return description_list\n",
    "\n",
    "\n",
    "labeled_data['processed_description'] = process_descriptions(labeled_data['short_description'])\n",
    "unlabeled_data['processed_description'] = process_descriptions(unlabeled_data['short_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 10 rows of the processed labeled data are as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>processed_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "      <td>[påskekrim, is, mer, the, tip, of, the, prover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "      <td>[madonn, is, slink, her, way, into, footwear, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "      <td>[but, what, if, you, re, a, 30, some, coupl, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "      <td>[obamac, wa, suppo, to, mak, bir, control, fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>Madonna previously released a Truth or Dare fr...</td>\n",
       "      <td>[madonn, prevy, relea, a, tru, or, dar, fragr,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                  short_description  \\\n",
       "0          TRAVEL  Påskekrim is merely the tip of the proverbial ...   \n",
       "2  STYLE & BEAUTY  Madonna is slinking her way into footwear now,...   \n",
       "3          TRAVEL  But what if you're a 30-something couple that ...   \n",
       "4        BUSINESS  Obamacare was supposed to make birth control f...   \n",
       "5  STYLE & BEAUTY  Madonna previously released a Truth or Dare fr...   \n",
       "\n",
       "                               processed_description  \n",
       "0  [påskekrim, is, mer, the, tip, of, the, prover...  \n",
       "2  [madonn, is, slink, her, way, into, footwear, ...  \n",
       "3  [but, what, if, you, re, a, 30, some, coupl, t...  \n",
       "4  [obamac, wa, suppo, to, mak, bir, control, fre...  \n",
       "5  [madonn, prevy, relea, a, tru, or, dar, fragr,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the first 10 rows of the processed unlabeled data are as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>short_description</th>\n",
       "      <th>processed_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Now, we're not exactly saying Kate is cutting ...</td>\n",
       "      <td>[now, we, re, not, exactl, say, kat, is, cut, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Instagram's Local Lens series is the perfect w...</td>\n",
       "      <td>[instagram, s, loc, len, ser, is, the, perfect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Check out the video on how to get our favorite...</td>\n",
       "      <td>[check, out, the, video, on, how, to, get, our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Want to meet the flesh-and-blood Annie Oakley?...</td>\n",
       "      <td>[want, to, meet, the, flesh, and, blood, ann, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>The latest line might be coming to us from Cha...</td>\n",
       "      <td>[the, latest, lin, might, be, com, to, us, fro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                  short_description  \\\n",
       "0      0  Now, we're not exactly saying Kate is cutting ...   \n",
       "1      1  Instagram's Local Lens series is the perfect w...   \n",
       "4      4  Check out the video on how to get our favorite...   \n",
       "5      5  Want to meet the flesh-and-blood Annie Oakley?...   \n",
       "6      6  The latest line might be coming to us from Cha...   \n",
       "\n",
       "                               processed_description  \n",
       "0  [now, we, re, not, exactl, say, kat, is, cut, ...  \n",
       "1  [instagram, s, loc, len, ser, is, the, perfect...  \n",
       "4  [check, out, the, video, on, how, to, get, our...  \n",
       "5  [want, to, meet, the, flesh, and, blood, ann, ...  \n",
       "6  [the, latest, lin, might, be, com, to, us, fro...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Balancing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow, the number of emails in dataset labeled as each of the categories is shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emails labeled as TRAVEL: 8461\n",
      "Number of emails labeled as BUSINESS: 4568\n",
      "Number of emails labeled as STYLE & BEAUTY: 8674\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of emails labeled as TRAVEL: {}\".format(len(labeled_data[(labeled_data['category'] == 'TRAVEL')])))\n",
    "print(\"Number of emails labeled as BUSINESS: {}\".format(len(labeled_data[(labeled_data['category'] == 'BUSINESS')])))\n",
    "print(\"Number of emails labeled as STYLE & BEAUTY: {}\".format(len(labeled_data[(labeled_data['category'] == 'STYLE & BEAUTY')])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In can be seen that the number of emails labeled as 'TRAVEL' is roughly the same as 'STYLE & BEAUTY' but dramatically bigger than 'BUSINESS'. This may cause future problems including a low performance of the classifier on detecting 'STYLE & BEAUTY' emails. For preventing such problems, we use a technique called **random oversampling** for artificially increasing the instances of the 'STYLE & BEAUTY' category.\n",
    "\n",
    "Random oversampling involves randomly duplicating examples from the minority class and adding them to the training dataset. Here, the minority class is 'BUSINESS' and so we randomly duplicate 4000 of its samples to increase its population to near the population of two other classes and balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLE=4000\n",
    "duplicated_samples = labeled_data[(labeled_data['category'] == 'BUSINESS')].sample(N_SAMPLE)\n",
    "labeled_data=labeled_data.append(duplicated_samples).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is almost balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emails labeled as TRAVEL: 8461\n",
      "Number of emails labeled as BUSINESS: 8568\n",
      "Number of emails labeled as STYLE & BEAUTY: 8674\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of emails labeled as TRAVEL: {}\".format(len(labeled_data[(labeled_data['category'] == 'TRAVEL')])))\n",
    "print(\"Number of emails labeled as BUSINESS: {}\".format(len(labeled_data[(labeled_data['category'] == 'BUSINESS')])))\n",
    "print(\"Number of emails labeled as STYLE & BEAUTY: {}\".format(len(labeled_data[(labeled_data['category'] == 'STYLE & BEAUTY')])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Splitting the Dataset to Train and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification tasks, the dataset is usually split into two subsets of training and test data. \n",
    "\n",
    "The training set is the actual dataset that we use to train the model (in the naïve Bayes classifies, this is equal to computing probabilities required in equation (5))). The model sees and learns from this data to be generalized to other data later on. \n",
    "\n",
    "The test set is a subset of data used to provide an unbiased evaluation of a model fit on the training dataset. The test set is used for measuring how the model can be generalized to other instances.\n",
    "\n",
    "Here, we consider 80% of the data as a training set while 20% as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PROPORTION = 0.8\n",
    "\n",
    "train_set = labeled_data.sample(frac=TRAIN_DATA_PROPORTION).copy()\n",
    "test_set = labeled_data.copy().drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9996109706282823"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)/len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three below criteria are used for evaluating the Naïve Bayes classifier:\n",
    "\n",
    "**1- Recall**\n",
    "\n",
    "For each class, Recall is defined as the ratio of the instances classified correctly in the class to the total number of instances in the class:\n",
    "\n",
    "$$Recall\\ for\\ C_k = \\frac {Number\\ of\\ Instanses\\ Corrrectly\\ Classified\\ as\\ C_k} {Number\\ of\\ Instanses\\ in\\ C_k}\\qquad(6)$$\n",
    "\n",
    "**2- Precision**\n",
    "\n",
    "For each class, Precision is defined as the ratio of the instances classified correctly in the class to the total number of instances classified as the class:\n",
    "\n",
    "$$Precision\\ for\\ C_k = \\frac {Number\\ of\\ Instanses\\ Corrrectly\\ Classified\\ as\\ C_k} {Number\\ of\\ Instanses\\ Classified\\ as\\ C_k}\\qquad(7)$$\n",
    "\n",
    "<a id=\"Q3\"></a>\n",
    "Due to the formula, the number of instances that belong to class $C_k$ but are misclassified is not counted in Precision. This fact makes Precision, alone, not sufficient for evaluating a classifier. It may misclassify lots for instances form $C_k$ but still get a good precision because most of its predictions for $C_k$ is correct.\n",
    "\n",
    "**3- Accuracy**\n",
    "\n",
    "Accuracy of the model is defined as the ratio of the instances classified correctly:\n",
    "\n",
    "$$Accuracy = \\frac {Number\\ of\\ Instanses\\ Classified\\ Corrrectly} {Total\\ Number\\ of\\ Instances}\\qquad(8)$$\n",
    "\n",
    "**4- The Confusion Matrix**\n",
    "\n",
    "In classification tasks, a confusion matrix is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The j'th element in i'th row of the matrix represents the number of instances, which belong to class j and classified as class i.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Applying the Naïve Bayes Classifiers to the Bag of Words Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we apply the Naïve Bayes classifying method for categorizing the dataset labels based on their short descriptions. We consider each of the words in a preprocessed email description as features and design a classifier for separating emails based on their description contents.\n",
    "\n",
    "\n",
    "In the beginning, we try to separate emails from the first two mentioned categories using Naïve Bayes, and finally, we apply the method for classifying all emails from three categories. In both parts, we then evaluate the obtained models with criteria from section (5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Computing the Probability for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use equation (5) for determining the class which each email belongs to, we must first find the probability of occasion for each class ($P(C_k)$ for each class $k$) and also the probability of single words to appear in a class ($P(x_i|C_k)$ for each feature $i$ and class $k$). \n",
    "\n",
    "An approximation for the probability of occasion for each class ($P(C_k)$ for each class $k$) is as follow:\n",
    "\n",
    "$$\\hat{P}(C_k) = \\frac {Number\\ of\\ instances\\ from\\ C_k} {Total\\ number\\ of\\ training\\ samples}\\qquad(9)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_category_probability = len(train_set[train_set['category'] == 'TRAVEL'])/len(train_set)\n",
    "business_category_probability = len(train_set[train_set['category'] == 'BUSINESS'])/len(train_set)\n",
    "style_category_probability = len(train_set[train_set['category'] == 'STYLE & BEAUTY'])/len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Computing the Relative Probability of Words in Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1. Using the Empirical Probabilities With Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of estimating the conditional probability of feature $x_i$ to appear in class $C_k$ ($P(x_i|C_k)$) is estimating them with their imperial probability as bellow:\n",
    "\n",
    "$$\\hat{P}(x_i | C_k) = \\frac {Times\\ that\\ x_i\\ appeared\\ in\\ instances\\ from\\ C_k} {Total\\ number\\ of\\ words\\ in\\ training\\ samples}\\qquad(10)$$\n",
    "\n",
    "This is the right idea, but there's a small problem:  what if there's a word $x$ form a new description belong to class $C_k$ that we've not seen before in $C_k$ instances from train set? In that case, $P(x|C_k) = 0$, and the entire probability for the email to be labeled as $C_k$ will go to zero (see equation 5). Similarly, lots of new emails out of our train set can easily get misclassified because only one word from their description is not in the true class words list.\n",
    "\n",
    "We would like our classifier to be robust to words it has not seen before. To address this problem, we must never let any word's probabilities to be zero, by smoothing the probabilities upwards. The solution is applying additive smoothing to the probabilities. \n",
    "\n",
    "In statistics, additive smoothing, also called Laplace smoothing or Lidstone smoothing, is a technique used to solve the problem of zero probability occasion. \n",
    "\n",
    "Given an observation $x  = ( x_1 , x_2 , … , x_d )$  from a multinomial distribution with $N $ trials, a smoothed version of the data gives the estimator:\n",
    "\n",
    "$$ \\hat{P}(x_i) = \\frac {x_i + α} {N + \\alpha d},\\qquad ( i = 1 , … , d )$$\n",
    "\n",
    "where the pseudo count α > 0 is a smoothing parameter. α = 0 corresponds to no smoothing.  Additive smoothing is a type of shrinkage estimator, as the resulting estimate will be between the empirical probability (relative frequency) $\\frac{x_i} {N}$, and the uniform probability $\\frac {1}{d}$.\n",
    "\n",
    "Using additive smoothing with $\\alpha=0$, the conditional probability of feature $x_i$ to appear in class $C_k$ ($P(x_i|C_k)$) would be:\n",
    "\n",
    "$$P(x_i | C_k) = \\frac {Times\\ that\\ x_i\\ appeared\\ in\\ instances\\ from\\ C_k +1} {Total\\ number\\ of\\ words\\ in\\ training\\ samples + 3}\\qquad(11)$$\n",
    "\n",
    "<a id=\"Q4\"></a>\n",
    "Applying the above formula for estimating probabilities, can prevent the classifier from mislabeling an instance only because one of its words has only appeared in the wrong class. Instead, there would be a change for other instances' words to affect the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2. Using TF-IDF <a id=\"Q2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach for estimating relative probabilities is using the **TF-IDF** measure of each word. **TF-IDF** stands for **term frequency-inverse document frequency** and is a statistical measure used to evaluate how important a word is to a document in a collection. The importance increases proportionally to **the number of times a word appears in the document** but is offset by the **frequency of the word in the collection**.\n",
    "\n",
    "The goal of using TF-IDF instead of the empirical probabilities with smoothing is to scale down the impact of words that occur very frequently in different classes and that are hence empirically less informative than features that occur in a small fraction of the training set.\n",
    "\n",
    "For applying TF-IDF in the Naïve Bayes classifier, we compute the term frequency (TF) for each of the words using equation (11) and then we modify it by multiplying it by an IDF factor bellow:\n",
    "\n",
    "$$IDF = log \\frac{Number\\ of\\ classes} {Number\\ of\\ classes\\ containing\\ the\\ word}$$\n",
    "\n",
    "\n",
    "Therefore, the TF-IDF measure for each word in the train set can be calculated using the eqaution bellow:\n",
    "\n",
    "\n",
    "$$TF-IDF(x_1|C_k) = \\frac {Times\\ that\\ x_i\\ appeared\\ in\\ instances\\ from\\ C_k +1} {Total\\ number\\ of\\ words\\ in\\ training\\ samples + 3} * log\\ \\frac{3} {Number\\ of\\ classes\\ containing\\ x_i}\\qquad(12)$$\n",
    "\n",
    "\n",
    "\n",
    "Finally, we can estimate the conditional probability $P(x_i | C_k)$ in equation (5) with $TF-IDF(x_1|C_k)$ and estimate the category of each email. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3. Implementing Relative  Probabilities in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code, we use equation (11) for calculating relative probabilities. The class bellow is defined to estimate the relative empirical relative probabilities of each word in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_relative_probabilities:\n",
    "    \n",
    "    \n",
    "    def __init__(self, class_probability, list_of_sentences, n_classes):\n",
    "        \n",
    "        self.__log_class_probability = log(class_probability)\n",
    "        \n",
    "        n_total_words = sum([len(sentence) for sentence in list_of_sentences])\n",
    "        self.__log_probability_denominator = log(n_total_words+n_classes)\n",
    "        \n",
    "        words_frequency = {word:1 for sentence in list_of_sentences for word in sentence}\n",
    "            \n",
    "        for sentence in list_of_sentences:\n",
    "            for word in sentence:\n",
    "                words_frequency[word] += 1\n",
    "\n",
    "        self.__log_words_probabilities = {key:log(words_frequency[key])-self.__log_probability_denominator for key in words_frequency.keys()}\n",
    "\n",
    "        \n",
    "    def __getitem__(self, word):\n",
    "        \n",
    "        try: \n",
    "            return self.__log_words_probabilities[word]\n",
    "        \n",
    "        except  KeyError:\n",
    "            return -self.__log_probability_denominator\n",
    "       \n",
    "    \n",
    "    def get_sentence_probability(self, sentence):\n",
    "        \n",
    "        return sum([self[word] for word in sentence]) + self.__log_class_probability\n",
    "        \n",
    "        \n",
    "          \n",
    "travel = log_relative_probabilities(class_probability=travel_category_probability,\n",
    "                                    list_of_sentences=train_set.loc[train_set['category'] == 'TRAVEL']['processed_description'],\n",
    "                                    n_classes=3)\n",
    "\n",
    "business = log_relative_probabilities(class_probability=business_category_probability,\n",
    "                                      list_of_sentences=train_set.loc[train_set['category'] == 'BUSINESS']['processed_description'],\n",
    "                                      n_classes=3)\n",
    "\n",
    "style = log_relative_probabilities(class_probability=style_category_probability,\n",
    "                                   list_of_sentences=train_set.loc[train_set['category'] == 'STYLE & BEAUTY']['processed_description'],\n",
    "                                   n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the above values, the class of each of the emails is determined using equation (5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Classifying Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow, there are codes for classifying test set emails by applying the Naïve Bayes classifier. Initially, we make attempts for classifying emails with \"Travel\" and \"Business\" labels from each other, and then we use the same method for categorizing all emails in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1. Classifying Emails With \"Travel\" and \"Business\" Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we select the test set instances which are labeled as \"Travel\" or \"Bussines\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_test_set = test_set.loc[(test_set['category'] == 'TRAVEL') ^ (test_set['category'] == 'BUSINESS')].copy()\n",
    "limited_test_set.assign(predicted_category=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the classification method to the limited test set and evaluate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in limited_test_set.index:\n",
    "\n",
    "    sentence = limited_test_set['processed_description'][idx]\n",
    "\n",
    "    travel_probability = travel.get_sentence_probability(sentence)\n",
    "    business_probability = business.get_sentence_probability(sentence)\n",
    "    \n",
    "    limited_test_set.loc[idx,'predicted_category'] = 'TRAVEL' if travel_probability>business_probability else 'BUSINESS'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the classifier with criteria from section 5.\n",
    "\n",
    "The value  of Accuracy is as follow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9079718640093787\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(sum(limited_test_set['category']==limited_test_set['predicted_category'])/len(limited_test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall and Precision criterium for different classes are show in the table bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Travel</th>\n",
       "      <th>Business</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.886837</td>\n",
       "      <td>0.928448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.923125</td>\n",
       "      <td>0.894386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Travel  Business\n",
       "Recall     0.886837  0.928448\n",
       "Precision  0.923125  0.894386"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase1_travel_recall = len(limited_test_set.loc[(limited_test_set['category'] == 'TRAVEL') & (limited_test_set['predicted_category'] == 'TRAVEL')])/len(limited_test_set.loc[limited_test_set['category'] == 'TRAVEL'])\n",
    "phase1_business_recall = len(limited_test_set.loc[(limited_test_set['category'] == 'BUSINESS') & (limited_test_set['predicted_category'] == 'BUSINESS')])/len(limited_test_set.loc[limited_test_set['category'] == 'BUSINESS'])\n",
    "\n",
    "phase1_travel_precision = len(limited_test_set.loc[(limited_test_set['category'] == 'TRAVEL') & (limited_test_set['predicted_category'] == 'TRAVEL')])/len(limited_test_set.loc[limited_test_set['predicted_category'] == 'TRAVEL'])\n",
    "phase1_business_precision = len(limited_test_set.loc[(limited_test_set['category'] == 'BUSINESS') & (limited_test_set['predicted_category'] == 'BUSINESS')])/len(limited_test_set.loc[limited_test_set['predicted_category'] == 'BUSINESS'])\n",
    "\n",
    "\n",
    "phase1_evaluation = pd.DataFrame({'Travel': [phase1_travel_recall, phase1_travel_precision], \n",
    "                                  'Business': [phase1_business_recall, phase1_business_precision]},\n",
    "                                  index=['Recall', 'Precision']) \n",
    "\n",
    "phase1_evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2. Classifying All Test Set Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we add an empty column to the new test set for saving the classifier's predicted categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.assign(predicted_category=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the classification method to the complete test set and evaluate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in test_set.index:\n",
    "    \n",
    "    sentence = test_set['processed_description'][idx]\n",
    "\n",
    "    travel_probability = travel.get_sentence_probability(sentence)\n",
    "    business_probability = business.get_sentence_probability(sentence)\n",
    "    style_probability = style.get_sentence_probability(sentence)\n",
    "    \n",
    "    if travel_probability>business_probability and travel_probability>style_probability:\n",
    "        test_set.loc[idx,'predicted_category'] = 'TRAVEL'\n",
    "    \n",
    "    elif business_probability>travel_probability and business_probability>style_probability:\n",
    "        test_set.loc[idx,'predicted_category'] = 'BUSINESS'\n",
    "    \n",
    "    else:\n",
    "        test_set.loc[idx,'predicted_category'] = 'STYLE & BEAUTY'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the classifier with criteria from section 5.\n",
    "\n",
    "The value of Accuracy is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8562536471503599\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(sum(test_set['category']==test_set['predicted_category'])/len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall and Precision criterium for different classes are show in the table bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Travel</th>\n",
       "      <th>Business</th>\n",
       "      <th>Style &amp; Beauty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.833830</td>\n",
       "      <td>0.896134</td>\n",
       "      <td>0.838057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.847971</td>\n",
       "      <td>0.841734</td>\n",
       "      <td>0.880851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Travel  Business  Style & Beauty\n",
       "Recall     0.833830  0.896134        0.838057\n",
       "Precision  0.847971  0.841734        0.880851"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase2_travel_recall = len(test_set.loc[(test_set['category'] == 'TRAVEL') & (test_set['predicted_category'] == 'TRAVEL')])/len(test_set.loc[test_set['category'] == 'TRAVEL'])\n",
    "phase2_business_recall = len(test_set.loc[(test_set['category'] == 'BUSINESS') & (test_set['predicted_category'] == 'BUSINESS')])/len(test_set.loc[test_set['category'] == 'BUSINESS'])\n",
    "phase2_style_recall = len(test_set.loc[(test_set['category'] == 'STYLE & BEAUTY') & (test_set['predicted_category'] == 'STYLE & BEAUTY')])/len(test_set.loc[test_set['category'] == 'STYLE & BEAUTY'])\n",
    "\n",
    "phase2_travel_precision = len(test_set.loc[(test_set['category'] == 'TRAVEL') & (test_set['predicted_category'] == 'TRAVEL')])/len(test_set.loc[test_set['predicted_category'] == 'TRAVEL'])\n",
    "phase2_business_precision = len(test_set.loc[(test_set['category'] == 'BUSINESS') & (test_set['predicted_category'] == 'BUSINESS')])/len(test_set.loc[test_set['predicted_category'] == 'BUSINESS'])\n",
    "phase2_style_precision = len(test_set.loc[(test_set['category'] == 'STYLE & BEAUTY') & (test_set['predicted_category'] == 'STYLE & BEAUTY')])/len(test_set.loc[test_set['predicted_category'] == 'STYLE & BEAUTY'])\n",
    "\n",
    "\n",
    "phase2_evaluation = pd.DataFrame({'Travel': [phase2_travel_recall, phase2_travel_precision], \n",
    "                                  'Business': [phase2_business_recall, phase2_business_precision],\n",
    "                                  'Style & Beauty': [phase2_style_recall, phase2_style_precision]},\n",
    "                                  index=['Recall', 'Precision']) \n",
    "\n",
    "\n",
    "phase2_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the confusion matrix is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Travel</th>\n",
       "      <th>Actual Business</th>\n",
       "      <th>Actual Style &amp; Beauty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted Travel</th>\n",
       "      <td>1400</td>\n",
       "      <td>98</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Business</th>\n",
       "      <td>165</td>\n",
       "      <td>1553</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Style &amp; Beauty</th>\n",
       "      <td>114</td>\n",
       "      <td>82</td>\n",
       "      <td>1449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Actual Travel  Actual Business  \\\n",
       "Predicted Travel                   1400               98   \n",
       "Predicted Business                  165             1553   \n",
       "Predicted Style & Beauty            114               82   \n",
       "\n",
       "                          Actual Style & Beauty  \n",
       "Predicted Travel                            153  \n",
       "Predicted Business                          127  \n",
       "Predicted Style & Beauty                   1449  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = pd.DataFrame({'Actual Travel': [len(test_set.loc[(test_set['category'] == 'TRAVEL') & (test_set['predicted_category'] == 'TRAVEL')]),\n",
    "                                                   len(test_set.loc[(test_set['category'] == 'TRAVEL') & (test_set['predicted_category'] == 'BUSINESS')]),\n",
    "                                                   len(test_set.loc[(test_set['category'] == 'TRAVEL') & (test_set['predicted_category'] == 'STYLE & BEAUTY')])], \n",
    "                                 'Actual Business': [len(test_set.loc[(test_set['category'] == 'BUSINESS') & (test_set['predicted_category'] == 'TRAVEL')]),\n",
    "                                                     len(test_set.loc[(test_set['category'] == 'BUSINESS') & (test_set['predicted_category'] == 'BUSINESS')]),\n",
    "                                                     len(test_set.loc[(test_set['category'] == 'BUSINESS') & (test_set['predicted_category'] == 'STYLE & BEAUTY')])],\n",
    "                                 'Actual Style & Beauty': [len(test_set.loc[(test_set['category'] == 'STYLE & BEAUTY') & (test_set['predicted_category'] == 'TRAVEL')]),\n",
    "                                                           len(test_set.loc[(test_set['category'] == 'STYLE & BEAUTY') & (test_set['predicted_category'] == 'BUSINESS')]),\n",
    "                                                           len(test_set.loc[(test_set['category'] == 'STYLE & BEAUTY') & (test_set['predicted_category'] == 'STYLE & BEAUTY')])]},\n",
    "                                index=['Predicted Travel', 'Predicted Business', 'Predicted Style & Beauty'])\n",
    "                                 \n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. The Effect of Preprocessing Stages on the Final Performance of the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1. Effect of Changing All the Letters to Their Lowercase Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was seen that changing all letters in the descriptions to their lowercase form increases the efficiency of the classifier. the reason for this is that words with mixed uppercase and lowercase letters vary form each other in shape and not in meaning. Despite this fact, Naïve Bayes makes the difference between these shapes of words and consider them as instances form different classes. Changing all letters in the descriptions solves this problem and consequently, it would enhance the performance of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2. Stemming Vs. Lemmatization  <a id=\"Q1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both stemming and lemmatization increase the performance of the classification task dramatically and the final classification evaluation results were almost the same while using either of them. (The difference of them was less than 3 percent for all evaluation methods). Bellow, we discuss some of the basic differences between these two methods.\n",
    "\n",
    "Both stemming and lemmatization try to bring inflected words to the same form. Stemming uses an algorithmic approach to removing prefixes and suffixes. The result might not be an actual dictionary word. On the other hand, lemmatization uses a corpus and so the result is always a dictionary word.\n",
    "\n",
    "This principle difference makes stemming much faster than lemmatizers but also less accurate as it only uses an algorithm for reducing inflections and not a valid dictionary. Another highlight fact about lemmatization is that it can group larger words with similar meanings together and usually can reduce the diversity of words more than stemming. (For more on stemming and lemmatization, please check section 4.4.2)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Classifying Unlabeled Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we classify the unlabeled emails using the preprocess operations with the highest performance in the previous section. The preprocessing steps for preparing the train, test, and unlabeled datasets are the ones that are written in the process_descriptions function in section 4.2.3 and are as follow:\n",
    "1. Changing all letters in the descriptions to their lowercase form\n",
    "2. Removing the stopwords\n",
    "3. Applying lemmatization\n",
    "\n",
    "Like previous sections, we add an empty column to the new test set for saving the classifier's predicted categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data.assign(category=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the classification method to the complete test set and evaluate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>short_description</th>\n",
       "      <th>processed_description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Now, we're not exactly saying Kate is cutting ...</td>\n",
       "      <td>[now, we, re, not, exactl, say, kat, is, cut, ...</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Instagram's Local Lens series is the perfect w...</td>\n",
       "      <td>[instagram, s, loc, len, ser, is, the, perfect...</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Check out the video on how to get our favorite...</td>\n",
       "      <td>[check, out, the, video, on, how, to, get, our...</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Want to meet the flesh-and-blood Annie Oakley?...</td>\n",
       "      <td>[want, to, meet, the, flesh, and, blood, ann, ...</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>The latest line might be coming to us from Cha...</td>\n",
       "      <td>[the, latest, lin, might, be, com, to, us, fro...</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>2543</td>\n",
       "      <td>Ironically the new taxes will have relatively ...</td>\n",
       "      <td>[iron, the, new, tax, wil, hav, rel, littl, ef...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>2544</td>\n",
       "      <td>Pack with purpose.</td>\n",
       "      <td>[pack, with, purpo]</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>2545</td>\n",
       "      <td>Despite some downtrodden Santas, there are 700...</td>\n",
       "      <td>[despit, som, downtrod, sant, ther, ar, 700, 0...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>2546</td>\n",
       "      <td>In the end, it doesn’t matter if your food is ...</td>\n",
       "      <td>[in, the, end, it, doesn, t, mat, if, yo, food...</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>2547</td>\n",
       "      <td>But changing taste brought trouble. Pulitzer c...</td>\n",
       "      <td>[but, chang, tast, brought, troubl, pulitz, cl...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2419 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                  short_description  \\\n",
       "0         0  Now, we're not exactly saying Kate is cutting ...   \n",
       "1         1  Instagram's Local Lens series is the perfect w...   \n",
       "4         4  Check out the video on how to get our favorite...   \n",
       "5         5  Want to meet the flesh-and-blood Annie Oakley?...   \n",
       "6         6  The latest line might be coming to us from Cha...   \n",
       "...     ...                                                ...   \n",
       "2543   2543  Ironically the new taxes will have relatively ...   \n",
       "2544   2544                                 Pack with purpose.   \n",
       "2545   2545  Despite some downtrodden Santas, there are 700...   \n",
       "2546   2546  In the end, it doesn’t matter if your food is ...   \n",
       "2547   2547  But changing taste brought trouble. Pulitzer c...   \n",
       "\n",
       "                                  processed_description        category  \n",
       "0     [now, we, re, not, exactl, say, kat, is, cut, ...  STYLE & BEAUTY  \n",
       "1     [instagram, s, loc, len, ser, is, the, perfect...          TRAVEL  \n",
       "4     [check, out, the, video, on, how, to, get, our...  STYLE & BEAUTY  \n",
       "5     [want, to, meet, the, flesh, and, blood, ann, ...          TRAVEL  \n",
       "6     [the, latest, lin, might, be, com, to, us, fro...  STYLE & BEAUTY  \n",
       "...                                                 ...             ...  \n",
       "2543  [iron, the, new, tax, wil, hav, rel, littl, ef...        BUSINESS  \n",
       "2544                                [pack, with, purpo]        BUSINESS  \n",
       "2545  [despit, som, downtrod, sant, ther, ar, 700, 0...        BUSINESS  \n",
       "2546  [in, the, end, it, doesn, t, mat, if, yo, food...          TRAVEL  \n",
       "2547  [but, chang, tast, brought, troubl, pulitz, cl...        BUSINESS  \n",
       "\n",
       "[2419 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx in unlabeled_data.index:\n",
    "    \n",
    "    sentence = unlabeled_data['processed_description'][idx]\n",
    "\n",
    "    travel_probability = travel.get_sentence_probability(sentence)\n",
    "    business_probability = business.get_sentence_probability(sentence)\n",
    "    style_probability = style.get_sentence_probability(sentence)\n",
    "    \n",
    "    if travel_probability>business_probability and travel_probability>style_probability:\n",
    "        unlabeled_data.loc[idx,'category'] = 'TRAVEL'\n",
    "\n",
    "    elif business_probability>travel_probability and business_probability>style_probability:\n",
    "        unlabeled_data.loc[idx,'category'] = 'BUSINESS'\n",
    "    \n",
    "    else:\n",
    "        unlabeled_data.loc[idx,'category'] = 'STYLE & BEAUTY'\n",
    "        \n",
    "unlabeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we extract the results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data[['index', 'category']].to_csv (r'output.csv', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
